# Tekstikaeve {#sec-tekstikaeve}

Andmed ei ole alati hoiustatud analüüsimiseks mugavates tabelites. Sageli on tulenevalt teabe olemusest palju loomulikum esitada andmed mõnel muul kujul. Üks näide sellest on teksti sisaldavad dokumendid. **Dokumentideks** võime selles tähenduses pidada ükskõik mis pikkusega tekste, alates sotsiaalmeedia sõnumitest ja lõpetades ilukirjanduslike teostega. Kuigi harjumuspärane ja loomulik on analüüsida dokumente neid lihtsalt lugedes, ei anna see nendest sageli piisavalt ülevaatlikku arusaamist. Nii on see eriti suurel arvul mahukate dokumentide korral. Tekstina esitatud teabe saab teisendada aga arvudeks, mis võimaldab läheneda teksti sisule kvantitatiivselt. Selleks on välja mõeldud mitmesuguseid protseduure ja neile vastavaid tööriistu, mille kasutamist nimetatakse andmeteadustes tekstikaeveks.

:::{.callout-important}
**Testikaeve** (*text mining*) seisneb dokumentide kvantitatiivses analüüsimises, mille käigus kirjeldatakse dokumente, tuvastatakse nendest teemad või leitakse seosed sõnade ja dokumentide vahel. Vastavad protseduurid põhinevad peamiselt sõnade või ka sõnapaaride esinemisel: sagedus, asukoht teiste sõnade suhtes ja esinemine dokumentides.
:::

Teksti kvantitatiivne uurimine võimaldab muuhulgas täita alljärgnevaid ülesandeid: 

- tekstis esinevate teemade tuvastamine,
- seoste otsimine tekstis esinevate sõnade ja väljendite vahel, 
- dokumentide klassifitseerimine ette antud klassidesse, 
- dokumentide klastritesse jaotamine, 
- teksti sisu näitlikustamine joonistel, 
- dokumendist kokkuvõtete tegemine.

Seega saame tekstikaevet kasutades vastata nt alljärgnevatele küsimustele: 

1. Mis on dokumendis sagedamini esinevad sõnad ja teemad?
2. Mille poolest üks teatud dokument erineb teistest?
3. Mis seosed esinevad dokumendis kasutatud väljendite vahel?

Mida rohkem on meil dokumente, seda selgemini ilmnevad seosed dokumentide vahel. Järgnevalt uurime teksikaeve meetodite rakendamist 2007. ja 2011. aasta Eesti erakondade valimisprogrammide näitel.

```{r}
library('magrittr')
failid <- list.files('andmed/cmp', full.names = TRUE, pattern = '.pdf$')
failid
```

Paljud dokumendid on kättesaadavaks tehtud .pdf vormingus dokumentidena. Selliste tekstide R keele objekti sisestamiseks saame kasutada funktsiooni `pdf_text()` laiendusest `pdftools`. Kasutades seda omakorda funktsioonis `sapply()`, saame mitu `.pdf` vormingus faili sisestada R töölauale vektorina, milles iga element on üks dokument

```{r}
library('pdftools')
dokumendid <- sapply(failid, pdf_text) %>% sapply(paste, collapse = ' ')
```

Teksti uurimiseks R keeles on loodud mitu laiendust, millest igaühel on oma lähenemine sellele, mis kujul tekste objektides hoiustada ja milliseid arvutusi tekstidega teha. Järgnevalt kasutame `quanteda` laienduste kogumikku, kuigi ära märkimist väärivad ka tekstikaeve jaoks mõeldud laiendused `tm` ja `tidytext`.

```{r}
library('quanteda')
library('quanteda.textstats')
library('quanteda.textplots')
```

## Korpus

Uuritavat dokumenti või ka mitme sellise dokumendi kogumit nimetatakse **korpuseks** (*corpus*). Laienduse quanteda funktsioonide kasutamiseks tuleks dokumente sisaldava vektor eelnevalt korpusena vormistatud objektiks teisendada funktsiooniga `corpus()`. 

```{r}
korpus <- corpus(dokumendid)
korpus
```

Korpuses olevatele dokumentidele võime määrata hilisemaks analüüsimiseks dokumentide nimed, aga ka muud tunnused. Antud juhul võikisme määrata igale dokumendile erakonna nimetuse ja aastaarvu.

```{r}
# Täpsustame dokumentide nimetused
erakonnad <- c('irl', 'kesk', 'rahvaliit', 'reform', 'rohe', 'sde')
names(korpus) <- paste(rep(erakonnad, each = 2), c(2007, 2011))
names(korpus)
# Määrame dokumentidele erakonna tunnuse
korpus$erakond <- rep(erakonnad, each = 2)
korpus$erakond
# Määrame dokumentidele aasta tunnuse
korpus$aasta <- c(2007, 2011)
korpus$aasta
```

## Sõnestamine

Paljude teksti kvantitaiivse analüüsimise protseduurid põhinevad sõnade sagedustel. Tihti on aga sõna liiga kitsas mõiste uuritava üksuse kirjeldamiseks, mistõttu on tekstikaeves analüüsi ühikuks tegelikult sõne (*token*). Sõne võib sisaldda ka rohkem kui ühte üksteisele piisavalt sagedasti järgnevat sõna. Sõnede uurimiseks tuleb esmalt korpuses esinevad dokumendid teisendada sõnede loeteludeks. Seda tegevust võib nimetada sõnestamiseks (*tokenization*). Sõnestada saame funktsiooniga `tokens()`, mille rakendamisel võime dokumentidest ühtlasi eemaldada ka kirjavahemärgid, sümbolid, arvud ja tühikud.

```{r}
sõned <- tokens(korpus, 
                remove_punct = TRUE, 
                remove_symbols = TRUE, 
                remove_numbers = TRUE)
```

Kuna suupärasem on rääkida sõnede asemel siiski sõnadest, siis edaspidi peame mõiste "sõna" all silmas tegelikult sõnesid.

Et mitte eristada sõnu suur- ja väiketähtede alusel, võiksime muuta kõik suurtähend väiketähtedeks.

```{r}
sõned %<>% tokens_tolower
sõned
```

Lisaks eelnevale võime eemaldada ka sõnad, milles on alla 2 tähemärgi.

```{r}
sõned %<>% tokens_select(min_nchar = 2)
sõned
```

Paljud tekstis sageli esinevad sõnad ei ole sisulised ega võimalda dokumenti temaatiliselt iseloomustada ega teistest tekstidest eristada. Sellised sõnad on nt sidesõnad ja asesõnad. Dokumentide analüüsimisel on need sõnad müra, mis raskendavad mustrite ja seoste leidmist. Tekstikaeves nimetatakse taolisi sõnu stoppsõnadeks (*stopwords*). Stoppsõnad saame sõnedest eemaldada funktsiooniga `tokens_remove()`. Inglisekeelsete stoppsõnad saame eemaldada argumendiga `pattern = stopwords('en')`. Eesti keele stoppsõnu aga vastavas funktsioonis ei ole, mistõttu peame need käsitsi arvutisse laadima^[Eesti keele stoppsõnad on alla laetavad [Tartu Ülikooli raamatukogust]( https://datadoi.ee/handle/33/78).], R töölauale sisestama ja seejärel vastava funktsiooni argumendina määrama.

```{r}
stoppsõnad <- readLines('andmed/stoppsõnad.txt')
sõned <- tokens_remove(sõned, pattern = stoppsõnad)
sõned
```

Üks ja sama sõna võib esineda mitmel erineval kujul, nt ainsuses ja mitmuses, eessõnadega ja eessõnadeta. Tegusõnadel on paljudes keeltes pöörded. Eesti keeles esinevad nimisõnad veel ka erinevates käänetes. Enne sõnede analüüsimist need seetõttu algvormistatakse (*lemmatizing*) ehk sõnad asendatakse nende algvormidega. Ingliskeelsete sõnede puhul saame kasutada funktsiooni `tokens_wordstem()`. Eestikeelsete sõnade algvormid ei ole paraku avalikult leitavad, mistõttu uuritavat korpust me algvormistada ei saa.

Võiksime arvesse võtta ka asjaolu, et teatud nähtused esitatakse rohkem kui ühe sõnana. Rohkem kui ühest sõnast koosnevad väljendid on **sõnaühendid** (*collocations*, *n-grams*). Dokumentides esinevate sõnade järjestuse alusel saame sõnaühendid leida funktsiooniga `textstat_collocations()`, milles saame määrata mh leitavate sõnaühendite väikseima sageduse (`min_count = 10`) ja vähina sõnade arvu, mille ühendeid otsime (`size = 2`).

```{r}
sõnaühendid <- textstat_collocations(sõned, min_count = 10, size = 2)
head(sõnaühendid)
```

Funktsioon `tokens_compound()` asendab korpuses eraldi sõnandena esinevad sõnad neile vastavate sõnaühenditega.

```{r}
sõned <- tokens_compound(sõned, sõnaühendid)
```

## Maatriksid

Sõnestatud korpuse alusel saame küll lihtsasti leida nt sõnade sagedused, aga huvitavamad mustrid ilmnevad sõnade ja dokumentide vaheliste seoste uurimisel. Selleks on vajalik korpus teisendada sageduste maatriksiks. Sellistes maatriksites nimetatkse sõnau sageli hoopis **terminiteks** (*term*). Nii võib eristada teistest ka harva esinevaid termineid (*sparse terms*), mis moodustavad tavaliselt suurema osa kõikidest terminitest.

**Dokumendi-termini maatriksis** (*document-feature matrix*) on ridades dokumendid ja veergudes kõik korpuses esinevad terminid. Lahtrites on seega vähemalt vaikimisi iga termini esinemissagedus igas dokumendis. 

```{r}
dokMat <- dfm(sõned)
dokMat
```

**Terminite maatriksis** (*feature co-occurrence matrix*) on ridades ja ka veergudes terminid. Lahtrites on nende terminite koos esinemised. Dokumentide uurimisel saame otsida seoseid kas 

- ühes dokumendis sõnade vahel lähtudes nende lähedusest või 
- mitme dokumendi vahel sõnade koos esinemise alusel.

Sealjuures on esimene lähenemine sobilikum üksikute pikkade dokumentide korral, viimane eeldab aga palju dokumente. Kasutades argumente `context = 'document'`, `count = 'boolean'` ja `window = 5`, loendame kui mitu korda esineb iga sõnade paar teineteisest kuni viie sõna kaugusel.

```{r}
termMat <- fcm(sõned, context = 'window', count = 'boolean', window = 5)
termMat
```

## Sõnade sagedused

Kõige lihtsam viis dokumentide kirjeldamiseks on lugeda kokku sõnad. Alustuseks võime võrrelda dokumente kõikide sõnade, kordumatute sõnade (*type*) ja ka lausete arvu alusel.

```{r}
ntoken(korpus) # Sõnade arv kokku
ntype(korpus) # Kordumatute sõnade arv
nsentence(korpus) # Lausete arv
```

Ülevaate tekstide sisust annavad **sõnasagedused** (*bag-of-words*, *word frequencies*), mis näitavad lihtsalt kui mitu korda sõna esineb. Kõige sagedamini esinevad sõnad annavad lihtsa ülevaate korpuse sisust.

```{r}
sage <- textstat_frequency(dokMat, n = 10)
sage
```

Neid sagedusi saame samuti uurida mingi korpuses esineva igat dokumenti esindava tunnuse lõikes.

```{r}
sageDok <- textstat_frequency(dokMat, groups = korpus$erakond, n = 3)
sageDok
```

Sõnasagedusi kujutatakse sageli **sõnapilvena** (*word cloud*). Sõnapilve suuruse saame määrata nt argumentidega `min_count` või `max_words`.

```{r}
textplot_wordcloud(dokMat, min_size = 1, max_words = 100)
```

Ka sõnapilvel saame esitada sagedused mingi tunnuse võrdluses.

```{r}
dokMatErak <- dfm_group(dokMat, groups = korpus$erakond)
textplot_wordcloud(dokMatErak, min_size = 1, max_words = 50, col = c('blue', 'red'), 
                   comparison = TRUE)
```

Saame valitud sõnade esinemist kujutada ka asukoha järgi tekstis, uurides sõnade konteksti (*keyword in context*, *kwic*). Kontekst on antud juhul dokumendid.

```{r}
textplot_xray(kwic(sõned, pattern = "Eesti"))
```

## Sõnasus

Konkreetseid dokumente saame temaatiliselt iseloomustada selle alusel, kuidas need erinevad sõnade esinemise poolest teistest dokumentidest. Seda erinemist võib nimetada **sõnasuseks** (*keyness*). Sõnasuse arvutamisel leitakse valitud mõõdiku alusel, millised sõnad esinevad valitud dokumendis sagedamini ja harvemini kui teistes dokumentides. Mõõdiku saame määrata argumendiga `measure` ja dokumendi määrada argumendi `target` abil. Tulemuse võime kuvada tulpjoonisel, määrates esitatavate sõnade arvu (`n`).

```{r}
sõnasus <-  textstat_keyness(dokMat, target = 'rohe 2007', measure = 'chi2')
textplot_keyness(sõnasus, n = 10)
```

## TF-IDF

Lisaks sõnasusele hinnatakse sagli dokumendis esineva sõna tähtsust igas dokumendis. Seda esindavat mõõdikut nimetatakse lühendiga **tf-idf** (*term frequency - inverse document frequency*). See arvutatakse ühe termini $t$ suhtelise sageduse ja dokumendi pöördsageduse korrutis.

$\text{tf-idf} (t) = \text{tf} (t) \times \text{idf} (t),$

kus suhteline sõnasagedus (*term frequency*) on sõna osakaal kõikidest sõnadest dokumendis: 

$\text{tf} (t) = \frac{\text{termini t sagedus dokumendis}}{\text{kõikide sõnade arv dokumendis}}$

ja dokumendi pöördsagedus (*inverse document frequency*) on sõna tähtsus terves korpuses: 

$\text{idf} (t) = ln \frac{\text{dokumentide arv}}{\text{terminit t sisaldavate dokumentide arv}}.$

Mõõdiku väärtus on seda kõrgem, mida sagedamini esineb termin dokumendis, aga mida harvemini teistes dokumentides. Mõni tavaline sõna võib olla küll dokumendis sage, aga kui see on sage ka teistes dokumentides, siis ei iseloomusta see sõna hästi just vaatlusalust dokumenti. Selleks on vajalik, et see sõna teistes dokumentides sage ei oleks. Niisiis näitab tf-idf sõna tähtsust dokumendis, arvestades sealjuures ka sõna üldist tähtsust. 

```{r}
tfidf <- dfm_tfidf(dokMat)
tfidf
```

## Sõnade võrgustik

Seosed sõnade vahel tulevad kõige ilmekamalt esile võrgustikjoonisel. Et kuvatud sõnade arv ei oleks liiga suur ja joonis seega liiga kirju, peaksime esmalt tegema sõnade seast valiku. Kohane on valida nt kõige sagedasemad sõnad. Seda saame teha taaskord funktsiooniga `topfeatures()`, mille tulemuseks saadud sageduste nimetused saame valida terminite maatriksist funktsiooniga `fcm_select()`. 

```{r}
sagedased <- fcm_select(termMat, names(topfeatures(dokMat, 20)))
```

Seejärel saame vähendatud terminite maatriksis esinevad seosed kujutada võrgustikuna funktsiooni `textplot_network()` abil. Nii joonistatud võrgustikul on näitavad jooned sõnade vahel nende koos esinemisi vastavalt sellele, kuidas see on määratud terminite maatriksis.

```{r}
textplot_network(sagedased, min_freq = .5, edge_alpha = 0.2, edge_size = 2)
```

## Klasterdamine

Mõnikord võib olla vajalik dokumentide rühmitamine. Selleks saame kasuada hierarhilist klasterdamist. Dokumentide vahelised paariviisilised kaugused saame arvutada nt sõnasageduste alusel, millisel juhul on samu sõnu sama sagedasti kasutavad dokumendid on üksteisele lähedal. Dokumendi termine maatriksi alusel saame dokumentide läheduse leida funktsiooniga `textstat_dist()`. Leitud kauguste alusel saame arvutada ja joonistada klastripuu nii nagu iga teise kauguste maatriksi alusel.

```{r}
kaugused <- textstat_dist(dokMat) %>% as.dist
puu <- hclust(kaugused, method = 'ward.D2') %>% as.dendrogram()
plot(puu, type = 'triangle')
```

## Sõnakasutus

Sõnakasutust dokumentides saab mõõta mitmest küljest. Dokumendi lugemise keerukust saame hinnata tähemärkide, kõikide sõnade, keeruliste sõnade, silpide ja lausete arvu ja pikkuse ning nende omavaheliste suhete alusel. Võimalusi nende tegurite kombineerimiseks on väga palju, aga eestikeelsete tekstide korral on sobilik kasutada mõõdikuid, mis ei võta aluseks sõnade tähendusi ega silpide arvu. Võime võrrelda tekste näiteks keskmise lause pikkuse alusel sõnades.

```{r}
textstat_readability(korpus, measure = 'meanSentenceLength')
```

Keelelise keerukuse mõõtmiseks on samuti suur valik mõõdikuid, millest paljud lähtuvad ühel või teisel viisil kõikide sõnade ja kordumatute sõnade suhtest. Neist võib-olla kõige lihtsam TTR (*type-token ratio*) on lihtsalt kordumatute sõnade ja kõikide sõnade arvu suhe.

```{r}
textstat_lexdiv(dokMat, measure = 'TTR')
```

Lisaks keerukusele on levinud ka **meelestatuse** (*sentiment*) analüüs. Selle käigus määratakse igale sõnale, kas see väljendab negatiivset või positiivset emotsiooni. Selle määratluse alusel saame hinnata iga dokumendile meelestatuse. See protseduur on arusaadaval põhjusel väga levinud nt sotsiaalmeedia sõnumite uurimisel. Paraku ei ole eesti keele korpuse kohta veel olemas sõnastikku, milles oleks määratud iga sõna meelsus. Seetõttu antud dokumentides me meelestatust uurida ei saa.

## Teemad

Keerulisemad tekstikaeve protseduurid võimaldavad luua teemade mudeleid (*topic models*). Üks selline protseduur on LDA (*Latent Dirichlet Allocation*). See võimaldab dokumendi-termini maatriksist leida ette antud arvul teemadega seotud märksõnad ja määrata igale dokumendile kõige sobivama teema, mida need märksõnad esindavad.

```{r}
#| message: false
library('seededlda')
teemad <- textmodel_lda(dokMat, k = 5)
terms(teemad, 5)
topics(teemad)
```